{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4369ef-d837-4147-bdaf-807542e297ff",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "\n",
    "Calculate R-Squared by the formula \n",
    "\n",
    "## R-squared = SSregression / SStotal\n",
    "\n",
    "Where:\n",
    "\n",
    "SSregression is the sum of squares due to regression (explained sum of squares)\n",
    "\n",
    "SStotal is the total sum of squares\n",
    "\n",
    "## R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d4206-9fdf-4403-95c6-09fdb8c73770",
   "metadata": {},
   "source": [
    "# ANSWER 2\n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n",
    "\n",
    "The most vital difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the model and R-squared does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860d90e-c077-489e-9cb6-47fc1b908a6e",
   "metadata": {},
   "source": [
    "# ANSWER 3\n",
    "It is better to use Adjusted R-squared when there are multiple variables in the regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa144e-6f18-4220-8de4-d84afea8bd20",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "\n",
    "Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
    "\n",
    "Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb04ef-8e5f-41d3-b5c0-cab25eb61dba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ANSWER 5\n",
    "## Mean Absolute Error (MAE) Advantages :\n",
    "It is an easy to calculate evaluation metric.\n",
    "\n",
    "All the errors are weighted on the same scale since absolute values are taken.\n",
    "\n",
    "It is useful if the training data has outliers as MAE does not penalize high errors caused by outliers.\n",
    "\n",
    "It provides an even measure of how well the model is performing.\n",
    "## Mean Absolute Error (MAE) Disadvantages :\n",
    "Sometimes the large errors coming from the outliers end up being treated as the same as low errors.\n",
    "\n",
    "MAE follows a scale-dependent accuracy measure where it uses the same scale as the data being measured. Hence it cannot be used to compare series’ using different measures.\n",
    "\n",
    "One of the main disadvantages of MAE is that it is not differentiable at zero. Many optimization algorithms tend to use differentiation to find the optimum value for parameters in the evaluation metric.\n",
    "\n",
    "It can be challenging to compute gradients in MAE.\n",
    "\n",
    "## Mean Squared Error (MSE) Advantages :\n",
    "MSE values are expressed in quadratic equations. Hence when we plot it, we get a gradient descent with only one global minima.\n",
    "\n",
    "For small errors, it converges to the minima efficiently. There are no local minima.\n",
    "\n",
    "MSE penalizes the model for having huge errors by squaring them.\n",
    "\n",
    "It is particularly helpful in weeding out outliers with large errors from the model by putting more weight on them.\n",
    "## Mean Squared Error (MSE) Disadvantages :\n",
    "One of the advantages of MSE becomes a disadvantage when there is a bad prediction. The sensitivity to outliers magnifies the high errors by squaring them.\n",
    "\n",
    "MSE will have the same effect for a single large error as too many smaller errors. But mostly we will be looking for a model which performs well enough on an overall level.\n",
    "\n",
    "MSE is scale-dependent as its scale depends on the scale of the data. This makes it highly undesirable for comparing different measures.\n",
    "\n",
    "When a new outlier is introduced into the data, the model will try to take in the outlier. By doing so it will produce a different line of best fit which may cause the final results to be skewed.\n",
    "\n",
    "## Root Mean Squared Error (RMSE) Advantages :\n",
    "RMSE is easy to understand.\n",
    "\n",
    "It serves as a heuristic for training models.\n",
    "\n",
    "It is computationally simple and easily differentiable which many optimization algorithms desire.\n",
    "\n",
    "RMSE does not penalize the errors as much as MSE does due to the square root.\n",
    "## Root Mean Squared Error (RMSE) Disadvantages :\n",
    "Like MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases.\n",
    "\n",
    "One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly.\n",
    "\n",
    "RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the results on different test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111f9d1-57bf-4476-b73e-61dc6644b497",
   "metadata": {},
   "source": [
    "# ANSWER 6\n",
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\n",
    "\n",
    "lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "\n",
    "Lasso tends to do well if there are a small number of significant parameters and the others are close to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fcb73-1ae0-463b-afb7-a8429fe34b60",
   "metadata": {},
   "source": [
    "# ANSWER 7 \n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f50c29-2093-41e7-899d-90066f36f109",
   "metadata": {},
   "source": [
    "# ANSWER 8\n",
    "## limitations of regularized linear models\n",
    "Non-Linearity of the response-predictor relationships.\n",
    "\n",
    "Correlation of error terms.\n",
    "\n",
    "A non-constant variance of the error term \n",
    "\n",
    "Collinearity.\n",
    "\n",
    "Outliers and High Leverage Points.\n",
    "\n",
    "Linear Regression deals with continuous values whereas classification problems mandate discrete values.The second problem is regarding the shift in threshold value when new data points are added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e67ca3-6156-49e3-ab45-703df55f93a5",
   "metadata": {},
   "source": [
    "# ANSWER 9\n",
    "\n",
    "Model B because\n",
    "\n",
    "It is an easy to calculate evaluation metric.\n",
    "\n",
    "All the errors are weighted on the same scale since absolute values are taken.\n",
    "\n",
    "It is useful if the training data has outliers as MAE does not penalize high errors caused by outliers.\n",
    "\n",
    "It provides an even measure of how well the model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48dd0a-5f6f-4a54-9b5f-6e9710711f5c",
   "metadata": {},
   "source": [
    "# ANSWER 10\n",
    "\n",
    "Model B because\n",
    "\n",
    "Lasso tends to do well if there are a small number of significant parameters and the others are close to zero .Ridge works well if there are many large parameters of about the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd5989-f753-483c-acec-f06d5b4706f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
